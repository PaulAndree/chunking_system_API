# chunking_system_API


Esta API desarrollada con FastApi expone un endpoint "/process_doc" el cual es capaz de procesar un archivo (pdf, docx, txt) y segmentarlo semanticamente, devolviendo una lista de chunks. Los parametros posibles a configurar para la segmentacion son los siguientes:

```python
splitter_pdf = RollingWindowSplitter(
    encoder=encoder,
    dynamic_threshold=True,
    min_split_tokens=150,
    max_split_tokens=300,
    window_size=5,
    plot_splits=False,  
    enable_statistics=False 
```

游댳 min_split_tokens define el tama침o m칤nimo de un chunk en tokens.

游댳 max_split_tokens Define el tama침o m치ximo del chunk antes de forzar un corte.

游댳 window_size  Controla el overlap sem치ntico entre chunks.

游댳 dynamic_threshold  Esto ajusta din치micamente los cortes seg칰n la coherencia sem치ntica detectada por el encoder.


Docker image:

docker pull paulbarreda9/chunking-api:latest


El c칩digo para esta ingesta se basa fundamentalmente en [MinerU](https://github.com/opendatalab/MinerU) y [semantic-router](https://pypi.org/project/semantic-router/).  
MinerU es una herramienta de c칩digo abierto desarrollada por OpenDataLab, dise침ada para facilitar el an치lisis y procesamiento de documentos complejos, como art칤culos acad칠micos, informes t칠cnicos y libros de texto y llevarlos a formatos estructurados como Markdown y JSON. Utiliza modelos como DocLayout-YOLO para detectar y estructurar elementos del documento, incluyendo encabezados, tablas, texto y f칩rmulas. Elimina autom치ticamente elementos redundantes como encabezados, pies de p치gina y n칰meros de p치gina, preservando la coherencia sem치ntica del texto. Convierte f칩rmulas matem치ticas en formato LaTeX, facilitando su edici칩n y an치lisis. Detecta y extrae tablas, represent치ndolas en formato HTML para su posterior procesamiento

Por otro lado semantic-router es usado para implementar directamente la segmentaci칩n sem치ntica permitiendo configurar parametros de la segmentaci칩n.

Ejemplo para llamar a la API:

```python
 # for txt

import requests
url = "http://localhost:8071/process_doc"

response = requests.post(
    url,
    data={"type_name": "note", "content": content, "language": "es"}
)
chunks = response.json()
chunks
```

```python
 # for pdf

import requests
url =  "http://localhost:8071/process_doc"


response = requests.post(
    url,
    files={"file": open( "/home/peyzaguirre/notebooks/dev_chunking_system/table.pdf", "rb")},
    data={"type_name": "pdf","output_dir":"/home/peyzaguirre/notebooks/dev_chunking_system/output", "language":"es"}
)
chunks = response.json()
```

```python
 # for doc

response = requests.post(
    url,
    files={"file": open( "/home/peyzaguirre/notebooks/dev_chunking_system/Carta.docx", "rb")},
    data={"type_name": "doc","output_dir":"/home/peyzaguirre/notebooks/dev_chunking_system/output", "language":"es"}
)
chunks_doc = response.json()
chunks_doc[0]
```


## EVALUACI칍N


Partiendo de que uno de los principales objetivos de un sistema de recuperaci칩n en aplicaciones de IA es identificar y recuperar 칰nicamente los tokens relevantes para una consulta determinada, proponemos una estrategia de evaluaci칩n que eval칰a el rendimiento de la recuperaci칩n a nivel de token el cual eval칰a el rendimiento de la relevancia de recuperaci칩n mediante precisi칩n, recuperaci칩n e intersecci칩n sobre uni칩n ( [칤ndice Jaccard](https://en.wikipedia.org/wiki/Jaccard_index) ) a partir de los tokens recuperados .

Gneracion de conjunto de datos:

Para el experimento 1, hemos utilizado un dataset peque침o que consiste en una consuta generada manualmente sobre un documento de caracter legal, y un pasaje o parrafo del documento que responde a dicha consulta. Por ejemplo:

<pre> ```
Cosulta generada por un humano : "쯖uales son las reglas que las administraciones publicas deben ajustarse para  garantizar la identidad y contenido de las copias electr칩nicas o en papel? "
Extracto sacado del documento =   Para garantizar la identidad y contenido de las copias electr칩nicas o en papel, y por tanto su car치cter de copias aut칠nticas, las Administraciones P칰blicas deber치n ajustarse a lo previsto en el Esquema Nacional de Interoperabilidad, el Esquema Nacional de Seguridad y sus normas t칠cnicas de desarrollo, as칤 como a las siguientes reglas: 
a) Las copias electr칩nicas de un documento electr칩nico original o de una copia electr칩nica aut칠ntica, con o sin cambio de formato, deber치n incluir los metadatos que acrediten su condici칩n de copia y que se visualicen al consultar el documento. 
b) Las copias electr칩nicas de documentos en soporte papel o en otro soporte no electr칩nico susceptible de digitalizaci칩n, requerir치n que el documento haya sido digitalizado y deber치n incluir los metadatos que acrediten su condici칩n de copia y que se visualicen al consultar el documento. 
c) Las copias en soporte papel de documentos electr칩nicos requerir치n que en las mismas figure la condici칩n de copia y contendr치n un c칩digo generado electr칩nicamente u otro sistema 
...

``` </pre>

## M칄TRICAS


Para una consulta relacionada a un corpus espec칤fico, solo un subconjunto de tokens dentro de ese corpus ser치 relevante. Idealmente, un sistema de recuperaci칩n deber칤a recuperar exactamente y 칰nicamente los tokens relevantes para cada consulta en todo el corpus. 
La metrica Intersecci칩n sobre Uni칩n (IoU) es una m칠trica que considera no solo si se recuperan fragmentos relevantes, sino tambi칠n cu치ntos tokens irrelevantes, redundantes o distractores se recuperan.
