# chunking_system_API


Esta API desarrollada con FastApi expone un endpoint "/process_doc" el cual es capaz de procesar un archivo (pdf, docx, txt) y segmentarlo semanticamente, devolviendo una lista de chunks. Los parametros posibles a configurar para la segmentacion son los siguientes:

```python
splitter_pdf = RollingWindowSplitter(
    encoder=encoder,
    dynamic_threshold=True,
    min_split_tokens=150,
    max_split_tokens=300,
    window_size=5,
    plot_splits=False,  # set this to true to visualize chunking
    enable_statistics=False  # to print chunking stats
)
```

游댳 min_split_tokens define el tama침o m칤nimo de un chunk en tokens.
游댳 max_split_tokens Define el tama침o m치ximo del chunk antes de forzar un corte.
游댳 window_size  Controla el overlap sem치ntico entre chunks.
游댳 dynamic_threshold  Esto ajusta din치micamente los cortes seg칰n la coherencia sem치ntica detectada por el encoder.


El c칩digo para esta ingesta se basa fundamentalmente en [MinerU](https://github.com/opendatalab/MinerU) y [semantic-router](https://pypi.org/project/semantic-router/).  
MinerU es una herramienta de c칩digo abierto desarrollada por OpenDataLab, dise침ada para facilitar el an치lisis y procesamiento de documentos complejos, como art칤culos acad칠micos, informes t칠cnicos y libros de texto y llevarlos a formatos estructurados como Markdown y JSON. Utiliza modelos como DocLayout-YOLO para detectar y estructurar elementos del documento, incluyendo encabezados, tablas, texto y f칩rmulas. Elimina autom치ticamente elementos redundantes como encabezados, pies de p치gina y n칰meros de p치gina, preservando la coherencia sem치ntica del texto. Convierte f칩rmulas matem치ticas en formato LaTeX, facilitando su edici칩n y an치lisis. Detecta y extrae tablas, represent치ndolas en formato HTML para su posterior procesamiento

Por otro lado semantic-router es usado para implementar la segmentaci칩n sem치ntica de manera automatizada, ya que permite directamente configurar parametros de segmentaci칩n.

Ejemplo para llamar a la API:

```python
import requests
url = "http://localhost:8071/process_doc"

response = requests.post(
    url,
    data={"type_name": "note", "content": content, "language": "es"}
)
chunks = response.json()
chunks
```

```python
 # for pdf

import requests
url =  "http://localhost:8071/process_doc"


response = requests.post(
    url,
    files={"file": open( "/home/peyzaguirre/notebooks/dev_chunking_system/table.pdf", "rb")},
    data={"type_name": "pdf","output_dir":"/home/peyzaguirre/notebooks/dev_chunking_system/output", "language":"es"}
)
chunks = response.json()
```

```python
 # for doc
response = requests.post(
    "http://localhost:8071/process_doc",
    files={"file": open( "/home/peyzaguirre/notebooks/dev_chunking_system/Carta.docx", "rb")},
    data={"type_name": "doc","output_dir":"/home/peyzaguirre/notebooks/dev_chunking_system/output", "language":"es"}
)
chunks_doc = response.json()
chunks_doc[0]
```


Find the docker image at :

docker pull paulbarreda9/chunking-api:latest

