# chunking_system_API


Esta API desarrollada con FastApi expone un endpoint "/process_doc" el cual es capaz de procesar un archivo (pdf, docx, txt) y segmentarlo semanticamente, devolviendo una lista de chunks. Los parametros posibles a configurar para la segmentacion son los siguientes:

```python
splitter_pdf = RollingWindowSplitter(
    encoder=encoder,
    dynamic_threshold=True,
    min_split_tokens=150,
    max_split_tokens=300,
    window_size=5,
    plot_splits=False,  
    enable_statistics=False 
```

üîπ min_split_tokens define el tama√±o m√≠nimo de un chunk en tokens.

üîπ max_split_tokens Define el tama√±o m√°ximo del chunk antes de forzar un corte.

üîπ window_size  Controla el overlap sem√°ntico entre chunks.

üîπ dynamic_threshold  Esto ajusta din√°micamente los cortes seg√∫n la coherencia sem√°ntica detectada por el encoder.


Docker image:

docker pull paulbarreda9/chunking-api:latest


El c√≥digo para esta ingesta se basa fundamentalmente en [MinerU](https://github.com/opendatalab/MinerU) y [semantic-router](https://pypi.org/project/semantic-router/).  
MinerU es una herramienta de c√≥digo abierto desarrollada por OpenDataLab, dise√±ada para facilitar el an√°lisis y procesamiento de documentos complejos, como art√≠culos acad√©micos, informes t√©cnicos y libros de texto y llevarlos a formatos estructurados como Markdown y JSON. Utiliza modelos como DocLayout-YOLO para detectar y estructurar elementos del documento, incluyendo encabezados, tablas, texto y f√≥rmulas. Elimina autom√°ticamente elementos redundantes como encabezados, pies de p√°gina y n√∫meros de p√°gina, preservando la coherencia sem√°ntica del texto. Convierte f√≥rmulas matem√°ticas en formato LaTeX, facilitando su edici√≥n y an√°lisis. Detecta y extrae tablas, represent√°ndolas en formato HTML para su posterior procesamiento

Por otro lado semantic-router es usado para implementar directamente la segmentaci√≥n sem√°ntica permitiendo configurar parametros de la segmentaci√≥n.

Ejemplo para llamar a la API:

```python
 # for txt

import requests
url = "http://localhost:8071/process_doc"

response = requests.post(
    url,
    data={"type_name": "note", "content": content, "language": "es"}
)
chunks = response.json()
chunks
```

```python
 # for pdf

import requests
url =  "http://localhost:8071/process_doc"


response = requests.post(
    url,
    files={"file": open( "/home/peyzaguirre/notebooks/dev_chunking_system/table.pdf", "rb")},
    data={"type_name": "pdf","output_dir":"/home/peyzaguirre/notebooks/dev_chunking_system/output", "language":"es"}
)
chunks = response.json()
```

```python
 # for doc

response = requests.post(
    url,
    files={"file": open( "/home/peyzaguirre/notebooks/dev_chunking_system/Carta.docx", "rb")},
    data={"type_name": "doc","output_dir":"/home/peyzaguirre/notebooks/dev_chunking_system/output", "language":"es"}
)
chunks_doc = response.json()
chunks_doc[0]
```


## EVALUACI√ìN


Partiendo de que uno de los principales objetivos de un sistema de recuperaci√≥n en aplicaciones de IA es identificar y recuperar √∫nicamente los tokens relevantes para una consulta determinada, proponemos una estrategia de evaluaci√≥n que eval√∫a el rendimiento de la recuperaci√≥n a nivel de token el cual eval√∫a el rendimiento de la relevancia de recuperaci√≥n mediante precisi√≥n, recuperaci√≥n e intersecci√≥n sobre uni√≥n ( [√≠ndice Jaccard](https://en.wikipedia.org/wiki/Jaccard_index) ) a partir de los tokens recuperados .

Gneracion de conjunto de datos:

Para el experimento 1, hemos utilizado un dataset peque√±o que consiste en una consuta generada manualmente sobre un documento de caracter legal, y un pasaje o parrafo del documento que responde a dicha consulta. Por ejemplo:

<pre> ```
Cosulta generada por un humano : "¬øcuales son las reglas que las administraciones publicas deben ajustarse para  garantizar la identidad y contenido de las copias electr√≥nicas o en papel? "
Extracto sacado del documento =   Para garantizar la identidad y contenido de las copias electr√≥nicas o en papel, y por tanto su car√°cter de copias aut√©nticas, las Administraciones P√∫blicas deber√°n ajustarse a lo previsto en el Esquema Nacional de Interoperabilidad, el Esquema Nacional de Seguridad y sus normas t√©cnicas de desarrollo, as√≠ como a las siguientes reglas: 
a) Las copias electr√≥nicas de un documento electr√≥nico original o de una copia electr√≥nica aut√©ntica, con o sin cambio de formato, deber√°n incluir los metadatos que acrediten su condici√≥n de copia y que se visualicen al consultar el documento. 
b) Las copias electr√≥nicas de documentos en soporte papel o en otro soporte no electr√≥nico susceptible de digitalizaci√≥n, requerir√°n que el documento haya sido digitalizado y deber√°n incluir los metadatos que acrediten su condici√≥n de copia y que se visualicen al consultar el documento. 
c) Las copias en soporte papel de documentos electr√≥nicos requerir√°n que en las mismas figure la condici√≥n de copia y contendr√°n un c√≥digo generado electr√≥nicamente u otro sistema 
...

``` </pre>

## M√âTRICAS


### IOU

Para una consulta relacionada a un corpus espec√≠fico, solo un subconjunto de tokens dentro de ese corpus ser√° relevante. Idealmente, un sistema de recuperaci√≥n deber√≠a recuperar exactamente y √∫nicamente los tokens relevantes para cada consulta en todo el corpus. 
La metrica Intersecci√≥n sobre Uni√≥n (IoU) es una m√©trica que considera no solo si se recuperan fragmentos relevantes, sino tambi√©n cu√°ntos tokens irrelevantes, redundantes o distractores se recuperan.

\[
\text{IoU}_q(\mathbf{C}) = \frac{|t_e \cap t_r|}{|t_e| + |t_r| - |t_e \cap t_r|}
\]

Donde:
- \( t_e \): conjunto de tokens esperados o relevantes (ground truth).
- \( t_r \): conjunto de tokens recuperados por el sistema.
- \( q_i \): query
- \( C \): Chunked corpus

Interpretaci√≥n:   Si el sistema recupera exactamente los mismos tokens que los relevantes ‚Üí IoU = 1. Si no hay solapamiento ‚Üí IoU = 0

### Precision

\[
\text{Precision}_q(\mathbf{C}) = \frac{|t_e \cap t_r|}{|t_r|}
\]


### Recall

\[
\text{IoU}_q(\mathbf{C}) = \frac{|t_e \cap t_r|}{|t_e|}
\]
